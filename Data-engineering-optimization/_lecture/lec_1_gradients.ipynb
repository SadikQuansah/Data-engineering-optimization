{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "The derivative of a function is the rate of change or that function. For instance, if a function described the speed of a car, the derivative would describe the acceleration or deceleration.\n",
    "\n",
    "In practice, this translates to the tangent of the function:\n",
    "\n",
    "<img src=\"../assets/derivative_plot.png\" style=\"max-width:500px\" />\n",
    "\n",
    "For differentiable functions, you have derivatives everywhere, which look like this:\n",
    "\n",
    "Note that not all continuous functions are [differentiable](https://math.stackexchange.com/questions/386967/necessary-and-sufficient-conditions-for-differentiability), but a function must be continuous to be differentiable (recall the definition of continuous functions from module 1).\n",
    "\n",
    "You could use [Wolfram Alpha](https://www.wolframalpha.com/) to calculate derivatives of function, but within python there's Sympy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative 1\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import sympy as sy \n",
    "\n",
    "x, y, z = sy.symbols('x y z')\n",
    "\n",
    "sy.init_printing(use_unicode=True) \n",
    "\n",
    "sy.diff(sy.cos(x),x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative 2\n",
    "\n",
    "sy.diff(sy.exp(x**2),x) #derivative of e^x - is e^x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative 3\n",
    "\n",
    "expr = sy.exp(x*y*z)\n",
    "\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy.diff(expr, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial derivatives\n",
    "\n",
    "In a function of many inputs, we can take derivatives with respect to one of the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivative 1\n",
    "\n",
    "sy.diff(expr,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivative 2\n",
    "\n",
    "m, n, a, b = sy.symbols('m n a b')\n",
    "\n",
    "expr = (a*x + b)**m\n",
    "\n",
    "expr.diff((x,n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2nd derivatives** are the rate of change of the rate of change.\n",
    "\n",
    "In a moving car, velocity would be the function, acceleration the derivative, and \"jerk\" would be the second derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "sy.diff(expr,x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "In a function taking in a vector, the **gradient** is the vector of partial derivatives of all the inputs of a function.\n",
    "\n",
    "Here it is represented on a 3d field of a $f(x_1, X_2) = y$ function:\n",
    "\n",
    "<img src=\"../assets/gradient_field.png\" style=\"max-width:500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization\n",
    "\n",
    "Take a differentiable function and a starting point. The derivative will always point us \"where to go\" to find a minimum or maximum from where you are. This is called gradient descent or \"hill climbing\".\n",
    "\n",
    "From now on we'll only talk about minimizing functions, because $min(-f(x)) = max(f(x))$.\n",
    "\n",
    "Remember, a minimum has a derivative of 0 (the tangent gradient is flat at that point by definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimization\n",
    "sns.set()\n",
    "f_x = lambda x: (x**3)-4*(x**2)+6\n",
    "\n",
    "x = np.linspace(-1,4,1000)\n",
    "\n",
    "#Plot the curve\n",
    "plt.plot(x, f_x(x))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write functions to plot gradient descent and also calulate minimum using gradient descent algorithm. We will be passing x_start(starting value of x), iterations, learning rate as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimization continued\n",
    "def plot_gradient(x, y, x_vis, y_vis):\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(x_vis, y_vis, c = \"b\")\n",
    "    plt.plot(x, f_x(x), c = \"r\")\n",
    "    plt.title(\"Gradient Descent\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(x_vis, y_vis, c = \"b\")\n",
    "    plt.plot(x,f_x(x), c = \"r\")\n",
    "    plt.xlim([2.0,3.0])\n",
    "    plt.title(\"Zoomed in Figure\")\n",
    "    plt.show()\n",
    "    \n",
    "def gradient_iteration(x_start, iterations, learning_rate):\n",
    "    \n",
    "    # These x and y value lists will be used later for visualisation.\n",
    "    x_grad = [x_start]\n",
    "    y_grad = [f_x(x_start)]\n",
    "    # Keep looping until number of iterations\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Get the Slope value from the derivative function for x_start\n",
    "        # Since we need negative descent (towards minimum), we use '-' of derivative\n",
    "        x_start_derivative = - f_x_derivative(x_start)\n",
    "        \n",
    "        # calculate x_start by adding the previous value to \n",
    "        # the product of the derivative and the learning rate calculated above.\n",
    "        x_start += (learning_rate * x_start_derivative)        \n",
    "        \n",
    "        x_grad.append(x_start)\n",
    "        y_grad.append(f_x(x_start))\n",
    "\n",
    "    print (\"Local minimum occurs at: {:.2f}\".format(x_start))\n",
    "    print (\"Number of steps: \",len(x_grad)-1)\n",
    "    plot_gradient(x, f_x(x) ,x_grad, y_grad)\n",
    "\n",
    "# Now , let's run gradient_iteration function with\n",
    "# x_start = 0.5, iterations = 1000, learning rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimization continued\n",
    "f_x_derivative = lambda x: 3*(x**2)-8*x #Derivative of the function above (can use sympy or wolfram alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "gradient_iteration(0.5, 1000, 0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "gradient_iteration(0.5, 1000, 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_iteration(0.5, 1000, 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
