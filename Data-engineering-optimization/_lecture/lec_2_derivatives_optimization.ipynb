{"cells":[{"cell_type":"markdown","metadata":{"id":"AxsFWdmAL3s7"},"source":["# Finding the root of a mathematical function\n","\n","[Here](https://scipy-lectures.org/advanced/mathematical_optimization/) is an excellent resource from scipy on this lecture.\n","\n","Why find the root (points $x$ where $f(x) = 0$)? Because we can re-frame most optimization problems in terms of root-finding.\n","\n","Recall that the minimum of a function is wheree $f'(x) = 0$ so it's root-finding for the derivative of the function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4g5aqm5L3s_"},"outputs":[],"source":["# root-finding\n","import numpy as np\n","import scipy as sp\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znFBKuV4L3tA"},"outputs":[],"source":["# root-finding continued\n","\n","def f(x):\n","    return np.cos(x) - x\n","\n","x = np.linspace(-5, 5, 1000)\n","y = f(x)\n","\n","\n","fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n","ax.axhline(0, color='k')\n","ax.plot(x, y)\n","ax.set_xlim(-5, 5)\n","plt.show() \n","\n","#Root is at intersection "]},{"cell_type":"markdown","metadata":{"id":"qGxNTLINL3tB"},"source":["## Bisection\n","\n","One of the most common algorithms for numerical root-finding is *bisection*.\n","\n","To understand the idea, recall the well-known game where\n","\n","- Player A thinks of a secret number between 1 and 100  \n","- Player B asks if it’s less than 50  \n","  \n","  - If yes, B asks if it’s less than 25  \n","  - If no, B asks if it’s less than 75    \n","\n","And so on.\n","\n","This is bisection; It works for all sufficiently well behaved increasing continuous functions with $ f(a) < 0 < f(b) $."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Te7jP6jdL3tC"},"outputs":[],"source":["np.log2(100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvfpJdtIL3tC"},"outputs":[],"source":["#https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/running-time-of-binary-search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BB-iJfsIL3tD"},"outputs":[],"source":["# bisection - similar to binary search\n","# Bisection works on generally most functions \n","\n","import scipy.optimize as opt \n","\n","x_min = opt.bisect(f, -5, 5)\n","print(f\"min: {x_min}\")\n","\n","fig, ax = plt.subplots(1,1, figsize=(5,3))\n","ax.axhline(0, color='k')\n","ax.plot(x,y)\n","\n","# Underscore plugs from the previous input\n","ax.scatter([x_min],[0],c='r',s=100, zorder=10)\n","ax.set_xlim(-5,5)\n"]},{"cell_type":"markdown","metadata":{"id":"giSkfuaML3tE"},"source":["## Hybrid Methods\n","\n","A general principle of numerical methods is as follows:\n","\n","- If you have specific knowledge about a given problem, you might be able to exploit it to generate efficiency.  \n","- If not, then the choice of algorithm involves a trade-off between speed and robustness.  \n","\n","In practice, most default algorithms for root-finding, optimization and fixed points use *hybrid* methods.\n","\n","These methods typically combine a fast method with a robust method in the following manner:\n","\n","1. Attempt to use a fast method  \n","1. Check diagnostics  \n","1. If diagnostics are bad, then switch to a more robust algorithm  \n","\n","In `scipy.optimize`, the function `brentq` is such a hybrid method and a good default"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVV8aV5FL3tF"},"outputs":[],"source":["# hybrid method\n","opt.brentq(f, -5, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xer6bq0L3tF"},"outputs":[],"source":["# hybrid method cont.\n","%timeit opt.bisect(f, -5, 5)\n","%timeit opt.brentq(f, -5, 5)\n"]},{"cell_type":"markdown","metadata":{"id":"3-VOdnJ_L3tG"},"source":["## Minimizing a mathematical function\n","\n","Most numerical packages provide only functions for *minimization*.\n","\n","Maximization can be performed by recalling that the maximizer of a function $ f $ on domain $ D $ is\n","the minimizer of $ -f $ on $ D $.\n","\n","Minimization is closely related to root-finding: For smooth functions, interior optima correspond to roots of the first derivative.\n","\n","The speed/robustness trade-off described above is present with numerical optimization too.\n","\n","Unless you have some prior information you can exploit, it’s usually best to use hybrid methods.\n","\n","For constrained, univariate (i.e., scalar) minimization, a good hybrid option is `fminbound`.\n","\n","### Gradient Descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlmjJET0L3tG"},"outputs":[],"source":["# gradient descent\n","\n","def f(x):\n","    return 1 - np.sin(x) / x\n","\n","x = np.linspace(-20., 20., 1000)\n","y = f(x)\n","\n","fig, ax = plt.subplots(1,1,figsize=(5,5))\n","ax.plot(x,y);\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tx3eyFGnL3tG"},"source":["Multivariate local optimizers include `minimize`, `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, and `fmin_ncg`.\n","\n","Constrained multivariate local optimizers include `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`.\n","\n","See the [documentation](http://docs.scipy.org/doc/scipy/reference/optimize.html) for details."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9Z_QXWpL3tH"},"outputs":[],"source":["# gradient descent continued .\n","\n","x0 = 3 #Initial guess\n","#xmin = opt.minimize(f,x0, method='Nelder-Mead').x #Both return same results\n","xmin = opt.minimize(f,x0, method='BFGS').x\n","\n","\n","fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n","ax.plot(x, y)\n","ax.scatter(x0, f(x0), marker='o', s=300)\n","ax.scatter(xmin, f(xmin), marker='v', s=300,\n","           zorder=20)\n","ax.set_xlim(-20, 20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JurSKFXRL3tH"},"outputs":[],"source":["# gradient descent continued ..\n","x0 = 11 #Increased iterations\n","xmin = opt.minimize(f, x0,method='BFGS').x\n","#xmin = opt.minimize(f, x0,method='Nelder-Mead').x\n","\n","\n","fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n","ax.plot(x, y)\n","ax.scatter(x0, f(x0), marker='o', s=300)\n","ax.scatter(xmin, f(xmin), marker='v', s=300,\n","           zorder=20)\n","ax.set_xlim(-20, 20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GG2XfaLAL3tH"},"outputs":[],"source":["# gradient descent continued ..\n","x0 = 11\n","#xmin = opt.minimize(f, x0,method='BFGS').x\n","xmin = opt.minimize(f, x0,method='Nelder-Mead').x\n","\n","\n","fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n","ax.plot(x, y)\n","ax.scatter(x0, f(x0), marker='o', s=300)\n","ax.scatter(xmin, f(xmin), marker='v', s=300,\n","           zorder=20)\n","ax.set_xlim(-20, 20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcH4q-B7L3tI"},"outputs":[],"source":["# gradient descent continued ...\n","x0 = 11\n","\n","# We use 1000 iterations.\n","xmin = opt.basinhopping(f, x0, 1000).x \n","\n","fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n","ax.plot(x, y)\n","ax.scatter(x0, f(x0), marker='o', s=300)\n","ax.scatter(xmin, f(xmin), marker='v', s=300,\n","           zorder=20)\n","ax.set_xlim(-20, 20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQG7EY4aL3tI"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}